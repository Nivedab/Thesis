{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Identification and validation of weak ties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for the implementation of the graphical analysis inorder to identify the weak ties and also to validate the results.\n",
    "\n",
    "### Background Theory\n",
    "There are several algorithms of link prediction algorithms based on node, topology and social thoery: that can be used in the MYMK (Members You May Know) recommendation system. This work uses the concepts of social theory to determine the tie strength between two users. There are seven dimensions that has to be considered to accurately predict the tie strength they are : Intensity of interaction, Intimacy, reccency of interaction, reciprocity, emmotional suppport, structure of connections and social distance. The actual tie strength between two users in real life cannot be exactly modeled by the data that is available on the Xing platform. Thus we use topology and socail theory concepts to identify the weak ties.\n",
    "\n",
    "### Ground truths\n",
    "> There cannot exists an quantitative value between two users that represent the strength of the mutual connection between the users similar to that of a real world relation. However we can have a binary distinction between a strong and a weak tie in the network\n",
    "\n",
    "> Strong triadic closure is the property among three nodes A, B, and C, such that if a strong tie exists between A-B and A-C, there is a weak or strong tie between B-C\n",
    "\n",
    "> Local bridge is an edge that connects two subgraphs. This Local bridge is always a weak tie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "refer the video below:\n",
    "\n",
    "Let \"A\" be the user to who contact recommendations are to be made. \"A\" is connected to \"B\" and \"B\" is connected to \"E\".This implies that \"E\" is a second degree contact or \"A\". \n",
    "\n",
    "According to hypothesis, recommending \"B\" a connection with \"E\" is a bad idea if the connection between \"A\" & \"B\"  or between \"B\" and \"E\" is a local bridge (weak tie) and if there are no mutual friends between \"A\" and \"E\" apart from \"B\".\n",
    "\n",
    "Therefore, we have 3 conditions that needs to be implemented:\n",
    "> Condition 1 : E is a second degree contact for A.\n",
    "\n",
    "> Condition 2 : Either one of the connection between A and B OR between B and E should be a local bridge.\n",
    "\n",
    "> Condition 3 : There are no other mutual friends between A and E apart from B.\n",
    "\n",
    "Outcome after the application of all the above conditions: We get a list of connections (edges) that according to the hypothesis is NOT a good recommendation as they have very low likelihood of knowing each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"hypothesis.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"hypothesis.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetworkX is a python library that is used to analyze the graphs. It can import a txt file that contains a list of pair of nodes that form an edge. The library itself checks for duplication of connections. It is necessary to keep in mind if it a directeed or undirected graph before importing. In this case it is a undirected, unweighed, one dimensional graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 31135\n",
      "Number of edges: 642287\n",
      "Average degree:  41.2582\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# real dataset\n",
    "data_set = nx.read_edgelist('data/sample-ch2017.txt')\n",
    "no_node = nx.number_of_nodes(data_set)\n",
    "print (nx.info(data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total connections\n",
    "The formula used to find the total number of edges that can be created is ggiven by the formula f(n) = (n$*$(n-1))$/$2 \n",
    "This incldes both existing links and also the ones that can be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges if the graph was complete : 484678545.0\n"
     ]
    }
   ],
   "source": [
    "# number of edges if the graph was complete : f(n) = (n*(n-1))/2\n",
    "#cg = nx.complete_graph(no_node) # takes too long to execute thus hashed\n",
    "#print(nx.number_of_nodes(cg))\n",
    "\n",
    "no_edges_possible = (no_node * (no_node - 1)) / 2\n",
    "print(\"Number of edges if the graph was complete :\", no_edges_possible) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Bridges\n",
    "Local bridges are those connections that act as an connection between two communities that have some factor in common. Following snippet of code identifies all the local bridges in the network and the nodes that are involved in the formation of local bridges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Local bridges in 2017:  96038\n",
      "Number of nodes included in a local bridges in 2017: 30013\n"
     ]
    }
   ],
   "source": [
    "# Local bridges (lb) (no common contacts)\n",
    "lb = list(nx.local_bridges(data_set , with_span = False))\n",
    "print('Number of Local bridges in 2017: ', len(list(nx.local_bridges(data_set, with_span = False))))\n",
    "\n",
    "# To create a list of nodes that belong to local node. from [('a','b'), ('c','b')] to ['a','b','c','b']\n",
    "new_lb = []\n",
    "for tup in lb:\n",
    "    for element in tup:\n",
    "        new_lb.append(element)  \n",
    "        \n",
    "#nd is to remove duplicates\n",
    "nodes_in_lb = list(set(new_lb))\n",
    "\n",
    "print('Number of nodes included in a local bridges in 2017:', len(nodes_in_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition 1: Finding all the second degree connections (sdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find second degree contacts of a single node\n",
    "def find_nodes(graph, node, distance):\n",
    "     # get all nodes within distance around the query node\n",
    "     nodes = set(nx.ego_graph(graph, node, radius=distance))\n",
    "     # remove nodes that are not **at** the specified distance but closer: removes all the first degree contact\n",
    "     if distance > 1:\n",
    "      nodes -= set(nx.ego_graph(graph, node, radius=distance-1))\n",
    "     return list(nodes)\n",
    "\n",
    "# example implementation\n",
    "#ba = find_nodes(data_set, nodes_in_lb[4], 2) # reeturns a list of all the nodes tat are 2nd deg\n",
    "#print((len(ba)))\n",
    "#ba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition 2: Identifying all the sdc tha pass through a node that is a part of local bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second degree contacts that pass through the LB of a single node\n",
    "# two parameters that it takes are 1) the graph itself here data_set 2) one specific local bridge (pair of nodes)\n",
    "# it returns : predictive edges between the [second degree node, the node that is the part of LB]  in that same order\n",
    "# returned variable is a nested list with the order 2\n",
    "def seconddegree (data_set, node_pair):\n",
    "    b = node_pair[0]\n",
    "    sdc_through_lb = []\n",
    "    n = list(data_set.neighbors(node_pair[1]))\n",
    "    pair =[]\n",
    "    \n",
    "    for a in range(len(n)):\n",
    "        lis = []\n",
    "        lis.append(n[a])\n",
    "        lis.append(b)\n",
    "        sdc_through_lb.append(lis)\n",
    "    return sdc_through_lb\n",
    "\n",
    "# example implementation\n",
    "#vr = seconddegree(data_set, lb[0])\n",
    "#print(vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works !! to get all the second degree edges with their respective nodes ! \n",
    "# It applies the above mentioned function to all the pair of nodes that form a local bridge\n",
    "# returns nested list of order 3 . \n",
    "# non existing edges ! predicted false edges\n",
    "# second node in this return set (pe) is always the node that is the part of a local bridge\n",
    "def allnodesseconddegree (data_set, lb):\n",
    "    sdc = []\n",
    "    for a in range(len(lb)):\n",
    "        var = seconddegree(data_set, lb[a])\n",
    "        sdc.append(var)\n",
    "    return sdc\n",
    "\n",
    "lb_second_degree_nodes = allnodesseconddegree(data_set, lb)\n",
    "#lb_second_degree_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition 3: Identify the edges that have no mutual friend apart fromm the one that is a part of local bridge formation. Returns the edges that satisfy the hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges that satisfy the hypothesis: (needs to be cleaned for duplications, reverse sequence of nodes) 7086940\n"
     ]
    }
   ],
   "source": [
    "#now for the last condition ! No other mutual friend apart from ***\n",
    "# returns the nested list of all the edges(node pair) that have only one mutual friend that is a part of LB\n",
    "interseclist = []\n",
    "satisfy_hypothesis = []\n",
    "\n",
    "for a in range(len(lb_second_degree_nodes)):\n",
    "    for b in range(len(lb_second_degree_nodes[a])):\n",
    "        cc = lb_second_degree_nodes[a]\n",
    "        \n",
    "        for c in range(len(cc[b])):\n",
    "            dd = cc[b]\n",
    "            \n",
    "            if dd[0] != dd[1]:\n",
    "                n1 = set(list(data_set.neighbors(dd[0])))\n",
    "                n2 = set(list(data_set.neighbors(dd[1])))\n",
    "                intersection = list(n1.intersection(n2))\n",
    "                ls = []\n",
    "                \n",
    "                if len(intersection) == 1 :\n",
    "                    ls.append(dd[0])\n",
    "                    ls.append(dd[1])\n",
    "                    satisfy_hypothesis.append(ls)\n",
    "\n",
    "print(\"Number of edges that satisfy the hypothesis: (needs to be cleaned for duplications, reverse sequence of nodes)\", len(satisfy_hypothesis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing clean up!\n",
    "The resulting list of edges can contain some duplications and reversed lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned duplications: 3543470\n",
      "(cleaned for reversed nodes) Final number of edges satisfying the hypothesis 3343196\n"
     ]
    }
   ],
   "source": [
    "# remove exact replications : eg [a,b] & [a,b] then only one is kept \n",
    "sh_tuples = set(map(tuple, satisfy_hypothesis))  #need to convert the inner lists to tuples so they are hashable\n",
    "no_duplication = list(map(list, sh_tuples))\n",
    "print(\"cleaned duplications:\",len(no_duplication))\n",
    "\n",
    "# works : if . [a,b] and [b, a] are in the list then only one either [a,b] or [b,a] is selected\n",
    "temp = set(frozenset(x) for x in no_duplication)\n",
    "lst = [list(x) for x in temp]\n",
    "#print(len(lst))\n",
    "\n",
    "#to remove duplications: [a,a] should be discarded\n",
    "all_hypothesis_edges = []\n",
    "for a in range(len(lst)):\n",
    "    temp1 = lst[a]\n",
    "    list_holder = []\n",
    "    if temp1[0] != temp1[1]:\n",
    "        list_holder.append(temp1[0])\n",
    "        list_holder.append(temp1[1])\n",
    "        all_hypothesis_edges.append(list_holder)\n",
    "print(\"(cleaned for reversed nodes) Final number of edges satisfying the hypothesis\",len(all_hypothesis_edges))\n",
    "\n",
    "# all_hypothesis_edges is the final list to use ! all_hypothesis_edges give a list of edges that are predicted to not get connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing set : \n",
    "The network as of year 2017 is the training set and network as of year 2018 is the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 26276\n",
      "Number of edges: 672589\n",
      "Average degree:  51.1942\n",
      "Number of edges that can be established: 345200950.0\n"
     ]
    }
   ],
   "source": [
    "# import 2018 dataset\n",
    "data_set2 = nx.read_edgelist('data/sample-ch2018.txt')\n",
    "node = nx.number_of_nodes(data_set2)\n",
    "print (nx.info(data_set2))\n",
    "fn = (node * (node - 1)) / 2\n",
    "print('Number of edges that can be established:', fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the overlap of ties between the two sets 2017 and 2018 the list of edges can have a reversed directions as well. Thus we check for the overlap in both the directions and sum theem up to get the final number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges intersection length is: 622635\n"
     ]
    }
   ],
   "source": [
    "# to check the extra edges that was created in 2018 and did not exist in 2017\n",
    "edg2017 = list(data_set.edges)\n",
    "edges2018 = list(data_set2.edges)\n",
    "\n",
    "direct_intesection = list(set(edg2017) & set(edges2018)) # intersection between the two\n",
    "#print(len(direct_intesection))\n",
    "\n",
    "# reverse checks\n",
    "#print(edg2017[7])\n",
    "\n",
    "# to convert all the tuples to list so that they can then be reversed\n",
    "l = []\n",
    "for a in range(len(edg2017)):\n",
    "    ls = edg2017[a]\n",
    "    l.append(list(ls))\n",
    "\n",
    "# reversing the edg2017 list\n",
    "for a in range(len(l)):\n",
    "    l[a].reverse()\n",
    "\n",
    "# reverse checks\n",
    "#print(\"Reversed list:\", l[7])\n",
    "\n",
    "#converting the list back to tuples so that the intersectio can be found\n",
    "lis =[]\n",
    "for a in range(len(l)):\n",
    "    ls = l[a]\n",
    "    lis.append(tuple(ls))\n",
    "    \n",
    "#print(\"Reversed list:\",  lis[7])    \n",
    "   \n",
    "reversed_intersection = list(set(lis).intersection(set(edges2018)))\n",
    "#print(len(reversed_intersection))\n",
    "\n",
    "#intesecction of 2017 and 2018 \n",
    "total_intersection = reversed_intersection + direct_intesection\n",
    "print('Edges intersection length is:', len(total_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection between 2017 and 2018 dataset: 49954\n"
     ]
    }
   ],
   "source": [
    "# difference\n",
    "difference = list((set(edges2018)).difference(set(total_intersection)))\n",
    "print(\"Intersection between 2017 and 2018 dataset:\",len(difference))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following snippet we identify the edges that overlap with the list of links that satisfy the hypothesis. The result of this is the number of false positives of the hpothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges that satisfy hypothesis and were alsoo fouund in 2018 dataset:  1038\n"
     ]
    }
   ],
   "source": [
    "# cleaning up data before comparing with 2018 dataset\n",
    "\n",
    "edges = list(data_set2.edges)\n",
    "\n",
    "# straight nodes to match them all\n",
    "into_tuples = []\n",
    "for a in range(len(all_hypothesis_edges)):\n",
    "    ls = all_hypothesis_edges[a]\n",
    "    into_tuples.append(tuple(ls))\n",
    "    \n",
    "\n",
    "# straight list overrlap\n",
    "#using sets to solve time consuming stuff\n",
    "direct_overlap = list(set(into_tuples) & set(edges))\n",
    "#print(len(direct_overlap))\n",
    "#print(direct_overlap)\n",
    "\n",
    "\n",
    "# reversing the nodes to match them all\n",
    "# this reverse command operates on the list itself :: inplace reverse operation!!\n",
    "\n",
    "#print((all_hypothesis_edges[7])) # before reversing the list\n",
    "for a in range(len(all_hypothesis_edges)):\n",
    "    all_hypothesis_edges[a].reverse()\n",
    "#print((all_hypothesis_edges[7]))\n",
    "\n",
    "rev = []\n",
    "for a in range(len(all_hypothesis_edges)):\n",
    "    ls = all_hypothesis_edges[a]\n",
    "    rev.append(tuple(ls))\n",
    "\n",
    "\n",
    "# reversed list of lsst overlap\n",
    "#using sets to solve time consuming stuff\n",
    "reversed_overlap = list(set(rev) & set(edges))\n",
    "#print(len(reversed_overlap))\n",
    "#print(reversed_overlap)\n",
    "\n",
    "total_edges_overlap = direct_overlap + reversed_overlap\n",
    "print(\"Number of edges that satisfy hypothesis and were alsoo fouund in 2018 dataset: \",len(total_edges_overlap))\n",
    "# exports\n",
    "np.savetxt(\"edges_that_overlap.csv\", total_edges_overlap, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the reslut, 1.038 links out of 49.954 created were the false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post analysis\n",
    "\n",
    "After carefull considerations, any of the following three couls be a cause of connection.\n",
    "\n",
    "> customers lifetime on the Xing platform may corelate with the 1038 connections:: reason -- The strong ties itself may not be established yet so below a threshold number of contact this hypothesis may not be useful.\n",
    "\n",
    "> May have some similar attributes !! same school or university or something like that would form a cluster but an influential relation will not cause a cluster formation thus these connections could be one way relation as well.\n",
    "\n",
    "> Network is cut off during sampling !! There may be some other friends that are not included in the sample this will influence the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impact of sampling on the results can be found by checking if there are any other mutual friends. Thus the following code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the list that does not have any other mutual friends from the whole data set of friends:  101\n"
     ]
    }
   ],
   "source": [
    "# All the fiends from Xing database of the nodes that are a part of edges that satisfy the hypothesis (1.038)\n",
    "all_friends = nx.read_edgelist('data/all_friends_of_users_involved_in_1038.csv',  delimiter=',', nodetype=int, encoding=\"utf-8\")\n",
    "\n",
    "# edge list that satisfy the hypothesis\n",
    "hyp_edges = nx.read_edgelist('edges_that_overlap.csv', delimiter=',', nodetype=int, encoding=\"utf-8\")\n",
    "\n",
    "# 2018 dataset\n",
    "set2018 = nx.read_edgelist('data/sample-ch2018.txt', nodetype=int, encoding=\"utf-8\")\n",
    "\n",
    "# 2017 dataset\n",
    "set2017 = nx.read_edgelist('data/sample-ch2017.txt', nodetype=int, encoding=\"utf-8\")\n",
    "\n",
    "ed = list(hyp_edges.edges)\n",
    "\n",
    "li =[]\n",
    "\n",
    "for a in range (len(ed)):\n",
    "    aa = ed[a]\n",
    "    for b in range (len(aa)):\n",
    "        x = aa[0]\n",
    "        y = aa[1]\n",
    "        \n",
    "        set2018mx = set(set2018.neighbors(x))\n",
    "        set2018my = set(set2018.neighbors(y))\n",
    "        \n",
    "        afnx = set(all_friends.neighbors(x))\n",
    "        afny = set(all_friends.neighbors(y))\n",
    "        \n",
    "        set2017px = set(set2017.neighbors(x))\n",
    "        set2017py = set(set2017.neighbors(y))\n",
    "        \n",
    "        m = set2018mx & set2018my\n",
    "        n = afnx & afny\n",
    "        p = set2017px & set2017py\n",
    "        \n",
    "    l=[]\n",
    "    if len(p) == len(n):\n",
    "        l.append(x)\n",
    "        l.append(y)\n",
    "        li.append(l)\n",
    "\n",
    "print(\"length of the list that does not have any other mutual friends from the whole data set of friends: \",len(li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
